{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMuMpch11LLw5xiEO7xS2VE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS3QH-z9VufW"
      },
      "source": [
        "import torch\n",
        "import functools\n",
        "import numpy  as np\n",
        "from IPython.core.debugger import set_trace"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw7-s8X4Po9t"
      },
      "source": [
        "https://github.com/karpathy/micrograd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-L0Wj5YWCNG"
      },
      "source": [
        "# make base class\n",
        "class Tensor:\n",
        "    def __init__(self, data):\n",
        "        if type(data) != np.ndarray:\n",
        "            print('error type data %r' % data)\n",
        "            assert(False)\n",
        "        self.data = data        \n",
        "        self.grad = None\n",
        "        self._ctx = None\n",
        "    \n",
        "    def __repr__(self):\n",
        "        # tensor([1.]))\n",
        "        return f'tensor({self.data})'\n",
        "\n",
        "    def backward(self, fill = True):\n",
        "        \n",
        "        if self._ctx is None:\n",
        "            return\n",
        "\n",
        "        if self.grad is None and fill:\n",
        "            # iniy first grad ones\n",
        "            assert self.data.size == 1\n",
        "            self.grad = np.ones_like(self.data)\n",
        "        \n",
        "        assert(self.grad is not None)  \n",
        "\n",
        "        \"\"\"\n",
        "        _ctx.backward return from <class '__main__.Mul'> \n",
        "        and method Mul.backward return y * grad_out, x * grad_out\n",
        "        \n",
        "        \"\"\"      \n",
        "        grads = self._ctx.backward(self._ctx, self.grad)\n",
        "        if len(self._ctx.parents) == 1:\n",
        "            grads = [grads]\n",
        "       \n",
        "        for t, g in zip(self._ctx.parents, grads):\n",
        "            # t  Tensor\n",
        "            if g.shape != t.data.shape:\n",
        "                print('grad shape must match tensor shape')\n",
        "                assert(False)           \n",
        "            # add grad to Tensor\n",
        "            t.grad = g          \n",
        "            # t.backward(False)\n",
        "         \n",
        "\n",
        "\"\"\"\n",
        "https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
        "https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html#:~:text=beginner%2Fexamples_autograd%2Ftwo_layer_net_custom_function-,PyTorch%3A%20Defining%20New%20autograd%20Functions,PyTorch%20autograd%20to%20compute%20gradients.\n",
        "В прямом проходе мы получаем тензор, содержащий ввод и возврат\n",
        "Тензор, содержащий вывод. \n",
        "\n",
        "ctx - это объект контекста, который можно использовать\n",
        "хранить информацию для обратных вычислений. Вы можете кешировать произвольные\n",
        "объекты для использования в обратном проходе с помощью метода ctx.save_for_backward.\n",
        "\n",
        "\n",
        "При обратном проходе мы получаем тензор, содержащий градиент потери\n",
        "относительно выхода, и нам нужно вычислить градиент потерь относительно входа.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Function:\n",
        "\n",
        "    def __init__(self, *tensor):\n",
        "        self.parents = tensor\n",
        "        self.saved_tensors = []\n",
        "    \n",
        "    def save_for_backward(self, *x):\n",
        "        self.saved_tensors.extend(x)    \n",
        "\n",
        "    def apply(self, arg, *x):\n",
        "        \"\"\"\n",
        "        почему мы здесь а потому что fn.apply и передаем аргумент fn\n",
        "        a = Tensor(np.array([1]))\n",
        "        b = Tensor(np.array([3]))\n",
        "        \n",
        "        a.mul(b)\n",
        "        \n",
        "        arg: <class '__main__.Mul'> , fn\n",
        "        self.data -> Tensor.data->[1] a\n",
        "        [3] = b        \n",
        "        \n",
        "        \"\"\"\n",
        "     \n",
        "        ctx = arg(self, *x)         \n",
        "        ret = Tensor(arg.forward(ctx, self.data, *[t.data for t in x]))\n",
        "        ret._ctx = ctx\n",
        "        return ret\n",
        "\n",
        "\n",
        "def register(name, fn):\n",
        "    \"\"\"\n",
        "    class A:\n",
        "        print('hell')\n",
        "    a = A()\n",
        "    setattr(a, 'oo', lambda x: x *2)\n",
        "    a.oo(2)\n",
        "    >4\n",
        "\n",
        "    we add mul to class Tensor    \n",
        "\n",
        "    partialmethod(fumc, arg)\n",
        "    \"\"\"    \n",
        "    setattr(Tensor, name, functools.partialmethod(fn.apply, fn))\n",
        "\n",
        "\n",
        "class Mul(Function):\n",
        "    \"\"\"\n",
        "    out = x.mul.y\n",
        "    back\n",
        "    out/dx, out/dy\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, y):\n",
        "        ctx.save_for_backward(x, y)\n",
        "        return x * y\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_out):       \n",
        "        # set_trace()\n",
        "        x, y = ctx.saved_tensors       \n",
        "        return y * grad_out, x * grad_out\n",
        "\n",
        "register('mul', Mul)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mEllEyyXF0y"
      },
      "source": [
        "#  copy paste\n",
        "# balnck \n",
        "# class (Function):\n",
        "\n",
        "#     @staticmethod\n",
        "#     def forward(ctx, x, y):\n",
        "#         ctx.save_for_backward(x, y)\n",
        "#         return \n",
        "\n",
        "#     @staticmethod\n",
        "#     def backward(ctx, grad_out):     \n",
        "#         x, y = ctx.saved_tensors       \n",
        "#         return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53Boi4vMxp7i"
      },
      "source": [
        "a = Tensor(np.array([1]))\n",
        "b = Tensor(np.array([3]))\n",
        "c = a.mul(b)\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBCDrtQkBXhT",
        "outputId": "e602714c-3ac1-4b2a-b62c-8d29b8de0c03"
      },
      "source": [
        "c"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myZjvzaaEjIk",
        "outputId": "f0821ebc-dead-4e19-ebe3-67b744347961"
      },
      "source": [
        "a.grad, b.grad"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqFtyfCA9kXI"
      },
      "source": [
        "c.backward()"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7075vNVFKGWs",
        "outputId": "ad68d4f5-8868-4a2b-ee84-64279b913b37"
      },
      "source": [
        "a.grad, b.grad"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([3]), array([1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMv0Aur9Z5LH"
      },
      "source": [
        "a  = torch.tensor([1.], requires_grad=True)\n",
        "b  = torch.tensor([3.], requires_grad=True)\n",
        "c = a.matmul(b)\n",
        "c.backward()"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLgKXK-XEttW",
        "outputId": "80464ddf-8859-497b-c290-7a3c70fa9e88"
      },
      "source": [
        "c"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3., grad_fn=<DotBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUKYVoyaao4z",
        "outputId": "7280f50a-c980-4e68-f7c2-01484a4ec70d"
      },
      "source": [
        "a.grad, b.grad"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([3.]), tensor([1.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAKN7fe6Wsyz"
      },
      "source": [
        "class Add(Function):\n",
        "    \"\"\"sum\"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, y):\n",
        "        ctx.save_for_backward(x, y)\n",
        "        return x + y\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_out):     \n",
        "        x, y = ctx.saved_tensors       \n",
        "        return grad_out, grad_out\n",
        "\n",
        "\n",
        "class ReLU(Function):\n",
        "    \"\"\"relu\"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, in_val):\n",
        "        ctx.save_for_backward(in_val)\n",
        "        return np.maximum(in_val, 0)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_out):     \n",
        "        in_val = ctx.saved_tensors   \n",
        "        grad_out[in_val < 0] = 0   \n",
        "        return grad_out\n",
        "\n",
        "class Sum(Function):\n",
        "    \"\"\"sum\"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, y):\n",
        "        ctx.save_for_backward(x, y)\n",
        "        return x + y\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_out):     \n",
        "        x, y = ctx.saved_tensors       \n",
        "        return grad_out, grad_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcYRtlP2erin"
      },
      "source": [
        "https://docs.sympy.org/latest/tutorial/calculus.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_BrZGdZWswe"
      },
      "source": [
        "from sympy import *"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfCeNn2aWst_",
        "outputId": "9c9cd215-4723-430e-f4cb-a384a415fc20"
      },
      "source": [
        "x, y = symbols('x, y')\n",
        "diff(x+y, x)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSK9-qpUWsnH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YASnAV5EWskf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5NJ3M4JWBMT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}