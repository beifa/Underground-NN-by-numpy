{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "14hxZe85SZg1kGx1IRQSsx3MfoJ5LleI3",
      "authorship_tag": "ABX9TyMIRDJ73HZGobs1Z1JhjW+m"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS3QH-z9VufW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c56ffa7-aa85-42b9-b88c-1325546f720a"
      },
      "source": [
        "import torch\n",
        "import functools\n",
        "import numpy  as np\n",
        "from IPython.core.debugger import set_trace\n",
        "import torchvision\n",
        "from tqdm import trange\n",
        "\n",
        "%pylab inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6CT3qtxODD1"
      },
      "source": [
        "Функция регистр добавляет в класс тензор метод mul, т.к.\n",
        "мул наследуется от класса Function то он имеет метод apply\n",
        "именно туда мы и перейдем\n",
        "\n",
        "        - setattr(Tensor, name, functools.partialmethod(fn.apply, fn))\n",
        "        - fn это мул <class '__main__.Mul'>\n",
        "        - name = 'mul'\n",
        "        - затем попав в метод apply (fn.apply)\n",
        "            - arg(fn) это аргумент который мы передали в apply\n",
        "\n",
        "и получается что  ctx это класс MUl, вызывает класс Tensor и передаем в него \n",
        "        \n",
        "        - Tensor(arg.forward(ctx, self.data, *[t.data for t in x]))\n",
        "        - self this a = 1\n",
        "        - arg <class '__main__.Mul'>\n",
        "        - x = 3 or b\n",
        "        - self.data => 1.data=> self this tensor after .data get array [1]\n",
        "        - 'подставим'\n",
        "        - Mul.forward (x= ( self.data, y = *[t.data for t in x]))\n",
        "            - save x, y\n",
        "            - mul x * y \n",
        "- ctx это класс мул он передается для сохранения значений x, y\n",
        "(method save tensor)\n",
        "- затем мы сохраняем ctx  в переменную _ctx  класса тензор\n",
        "- затем если мы выполним метод backward in class Tensor\n",
        "произойдет следующие\n",
        "\n",
        "        - пока не вызвали(backward) в классе Tensor есть\n",
        "        только результат мул и сохраненный класс мул\n",
        "        - и так вызываем\n",
        "\n",
        "        if self._ctx is None:\n",
        "            return\n",
        "            проходим тк не нон\n",
        "        if self.grad is None and fill:\n",
        "            # iniy first grad ones\n",
        "            assert self.data.size == 1\n",
        "            self.grad = np.ones_like(self.data)      \n",
        "        assert(self.grad is not None) \n",
        "        \n",
        "        инициализируем градиент единицами, self.grad = [1]\n",
        "\n",
        "        grads = self._ctx.backward(self._ctx, self.grad)\n",
        "        \n",
        "        grads = (array([3]), array([1]))\n",
        "        - Mul.backward\n",
        "        \n",
        "            def backward(ctx, grad_out):      \n",
        "                x, y = ctx.saved_tensors =>  [array([1]), array([3])] \n",
        "                return y * grad_out, x * grad_out\n",
        "\n",
        "        получаем сохраненные значения изначальные\n",
        "        затем умножаем на градиент он равен 1 пока что\n",
        "        и возвращаем полученные значения\n",
        "        ctx.parents  это тензоры а и б\n",
        "            \n",
        "            if len(self._ctx.parents) == 1:\n",
        "                grads = [grads]\n",
        "\n",
        "        это сделано чтоб использовать зип\n",
        "            \n",
        "            - self._ctx.parents (tensor([1]), tensor([3])),\n",
        "            - grads = (array([3]), array([1]))\n",
        "\n",
        "            for t, g in zip(self._ctx.parents, grads):\n",
        "                # t  Tensor\n",
        "                if g.shape != t.data.shape:\n",
        "                    print('grad shape must match tensor shape')\n",
        "                    assert(False)           \n",
        "                t.grad = g"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-L0Wj5YWCNG"
      },
      "source": [
        "# make base class\n",
        "class Tensor:\n",
        "    def __init__(self, data):\n",
        "        if type(data) != np.ndarray:\n",
        "            set_trace()\n",
        "            print('error type data %r' % data)\n",
        "            assert(False)\n",
        "        self.data = data        \n",
        "        self.grad = None\n",
        "        self._ctx = None\n",
        "    \n",
        "    def __repr__(self):\n",
        "        # tensor([1.]))\n",
        "        return f'tensor({self.data})'\n",
        "\n",
        "    def backward(self, fill = True):\n",
        "        \n",
        "        # print(\"running backward on\", self)\n",
        "\n",
        "        if self._ctx is None:\n",
        "            return\n",
        "\n",
        "        if self.grad is None and fill:\n",
        "            # iniy first grad ones\n",
        "            assert self.data.size == 1\n",
        "            self.grad = np.ones_like(self.data)\n",
        "        \n",
        "        assert(self.grad is not None)  \n",
        "\n",
        "        \"\"\"\n",
        "        _ctx.backward return from <class '__main__.Mul'> \n",
        "        and method Mul.backward return y * grad_out, x * grad_out\n",
        "        \n",
        "        \"\"\"      \n",
        "        grads = self._ctx.backward(self._ctx, self.grad)\n",
        "        # print('Func name: ',type(self._ctx))\n",
        "        # print('Grad: ', self.grad.shape)\n",
        "        # print('Self: ', self.data.shape)\n",
        "        # set_trace()\n",
        "        if len(self._ctx.parents) == 1:\n",
        "            grads = [grads]\n",
        "\n",
        "        # print(self._ctx.parents, grads)\n",
        "\n",
        "        for t, g in zip(self._ctx.parents, grads):\n",
        "            # t  Tensor\n",
        "            # print(type(t.data), len(t.data.shape), t)\n",
        "            # if g.shape != t.data.shape:\n",
        "            #     print('grad shape must match tensor shape')\n",
        "            #     assert(False)\n",
        "            t.grad = g           \n",
        "            t.backward(False)          \n",
        "      \n",
        "         \n",
        "\n",
        "\"\"\"\n",
        "https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
        "https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html#:~:text=beginner%2Fexamples_autograd%2Ftwo_layer_net_custom_function-,PyTorch%3A%20Defining%20New%20autograd%20Functions,PyTorch%20autograd%20to%20compute%20gradients.\n",
        "В прямом проходе мы получаем тензор, содержащий ввод и возврат\n",
        "Тензор, содержащий вывод. \n",
        "\n",
        "ctx - это объект контекста, который можно использовать\n",
        "хранить информацию для обратных вычислений. Вы можете кешировать произвольные\n",
        "объекты для использования в обратном проходе с помощью метода ctx.save_for_backward.\n",
        "\n",
        "\n",
        "При обратном проходе мы получаем тензор, содержащий градиент потери\n",
        "относительно выхода, и нам нужно вычислить градиент потерь относительно входа.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Function:\n",
        "\n",
        "    def __init__(self, *tensor):\n",
        "        self.parents = tensor  \n",
        "        self.saved_tensors = []\n",
        "    \n",
        "    def save_for_backward(self, *x):\n",
        "        self.saved_tensors.extend(x)    \n",
        "\n",
        "    def apply(self, arg, *x):\n",
        "        \"\"\"\n",
        "        почему мы здесь а потому что fn.apply и передаем аргумент fn\n",
        "        a = Tensor(np.array([1]))\n",
        "        b = Tensor(np.array([3]))\n",
        "        \n",
        "        a.mul(b)\n",
        "        \n",
        "        arg: <class '__main__.Mul'> , fn\n",
        "        self.data -> Tensor.data->[1] a\n",
        "        [3] = b        \n",
        "        \n",
        "        \"\"\"        \n",
        "        ctx = arg(self, *x)         \n",
        "        ret = Tensor(arg.forward(ctx, self.data, *[t.data for t in x]))\n",
        "        ret._ctx = ctx\n",
        "        return ret\n",
        "\n",
        "\n",
        "def register(name, fn):\n",
        "    \"\"\"\n",
        "    class A:\n",
        "        print('hell')\n",
        "    a = A()\n",
        "    setattr(a, 'oo', lambda x: x *2)\n",
        "    a.oo(2)\n",
        "    >4\n",
        "\n",
        "    we add mul to class Tensor    \n",
        "\n",
        "    partialmethod(fumc, arg)\n",
        "    \"\"\"    \n",
        "    # set_trace()\n",
        "    setattr(Tensor, name, functools.partialmethod(fn.apply, fn))\n",
        "\n",
        "\n",
        "class Mul(Function):\n",
        "    \"\"\"\n",
        "    out = x.mul.y\n",
        "    back\n",
        "    out/dx, out/dy\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, y):\n",
        "        ctx.save_for_backward(x, y)\n",
        "        return x * y\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_out):       \n",
        "        # set_trace()\n",
        "        x, y = ctx.saved_tensors       \n",
        "        return y * grad_out, x * grad_out\n",
        "\n",
        "register('mul', Mul)\n",
        "\n",
        "class Add(Function):\n",
        "    \"\"\"sum\"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, y):\n",
        "        ctx.save_for_backward(x, y)\n",
        "        return x + y\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_out):     \n",
        "        x, y = ctx.saved_tensors       \n",
        "        return grad_out, grad_out\n",
        "\n",
        "\n",
        "class ReLU(Function):\n",
        "    \"\"\"relu\"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, in_val):\n",
        "        ctx.save_for_backward(in_val)\n",
        "        return np.maximum(in_val, 0)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_out):     \n",
        "        in_val = ctx.saved_tensors         \n",
        "        grad_out[in_val[0] < 0] = 0   \n",
        "        return grad_out\n",
        "\n",
        "register('relu', ReLU)        \n",
        "\n",
        "class Sum(Function):\n",
        "    \"\"\"sum array each dx is 1\"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, in_val):\n",
        "       \n",
        "        ctx.save_for_backward(in_val)\n",
        "        return np.array([in_val.sum()])\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_out):     \n",
        "        in_val, = ctx.saved_tensors #!!!!!!!!!!!!!!!!!!!!!!!!!!            \n",
        "        return grad_out * np.ones_like(in_val)\n",
        "\n",
        "\n",
        "register('sum', Sum)\n",
        "\n",
        "class Dot(Function):\n",
        "    \"\"\"\n",
        "    a = [[1, 0], [0, 1]]\n",
        "    b = [[4, 1], [2, 2]]\n",
        "    np.dot(a, b)\n",
        "    >> array([[4, 1],\n",
        "             [2, 2]])  \n",
        "    \n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, in_val, w):\n",
        "        ctx.save_for_backward(in_val, w)\n",
        "        return in_val.dot(w)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_out):     \n",
        "        in_val, w = ctx.saved_tensors\n",
        "        # set_trace()\n",
        "        grad_out = np.squeeze(grad_out)\n",
        "        g_grad = grad_out.dot(w.T) # (10,10,1) and (128,10)  after 10, 128\n",
        "        w_grad = grad_out.T.dot(in_val).T\n",
        "        return g_grad, w_grad\n",
        "\n",
        "register('dot', Dot)\n",
        "\n",
        "\n",
        "class Log_softmax(Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        ctx.save_for_backward(x)\n",
        "        mx = x.max(axis=1)\n",
        "        stbl = mx + np.log(np.exp(x- mx.reshape((-1, 1))).sum(axis=1))        \n",
        "        return x - stbl.reshape(-1,1)        \n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_out):     \n",
        "        x = ctx.saved_tensors\n",
        "        # set_trace() \n",
        "        return  grad_out - np.exp(x)*grad_out.sum(axis=1).reshape((-1, 1))      \n",
        " \n",
        "register('log_softmax', Log_softmax)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3mhhvLwrHLm",
        "outputId": "dfc3dc2e-5b4e-4d95-ed4c-68eded303894"
      },
      "source": [
        "a = Tensor(np.array([1]))\n",
        "b = Tensor(np.array([3]))\n",
        "c = a.mul(b)\n",
        "# check how works\n",
        "print('No grad: ', a.grad, b.grad)\n",
        "c.backward()\n",
        "print('Numpy grad: ', c)\n",
        "print('Numpy Grad: ', a.grad, b.grad)\n",
        "print('-----------------')\n",
        "print('Time to torch')\n",
        "a  = torch.tensor([1.], requires_grad=True)\n",
        "b  = torch.tensor([3.], requires_grad=True)\n",
        "c = a.matmul(b)\n",
        "c.backward()\n",
        "print('Torch grad: ', c)\n",
        "print('Torch Grad: ', a.grad, b.grad)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No grad:  None None\n",
            "Func name:  <class '__main__.Mul'>\n",
            "Grad:  (1,)\n",
            "Self:  (1,)\n",
            "Numpy grad:  tensor([3])\n",
            "Numpy Grad:  [3] [1]\n",
            "-----------------\n",
            "Time to torch\n",
            "Torch grad:  tensor(3., grad_fn=<DotBackward>)\n",
            "Torch Grad:  tensor([3.]) tensor([1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwI4cPYjs9qa"
      },
      "source": [
        "TODO:\n",
        "\n",
        "release train with tensor\n",
        "\n",
        "        from mnist.ipynb\n",
        "\n",
        "        np.random.seed(13)\n",
        "        lr = 0.001\n",
        "        batch = 30\n",
        "        #init weight\n",
        "        l1 = weight_init(784, 128, mode = 'uniform')\n",
        "        l2 = weight_init(128, 10, mode = 'uniform')\n",
        "\n",
        "        bar = trange(500)\n",
        "        accuracies, losses = [], []\n",
        "        for i in bar:\n",
        "            samp = np.random.randint(0, x.shape[0], size=(batch))\n",
        "            X = x[samp].reshape(-1, 28*28).float()\n",
        "            Y = y[samp].long()\n",
        "\n",
        "            x_loss, out, d_l1, d_l2 = backward(X.numpy(), Y.numpy(), l1, l2)\n",
        "\n",
        "            cat = np.argmax(out, axis = 1)\n",
        "            acc = (cat == Y.numpy()).mean()\n",
        "\n",
        "            # update weight,  SGD\n",
        "\n",
        "            l1 = l1 - lr * d_l1\n",
        "            l2 = l2 - lr * d_l2\n",
        "\n",
        "            x_loss = x_loss.mean()\n",
        "            accuracies.append(acc)\n",
        "            losses.append(x_loss)\n",
        "            bar.set_description('Loss:  %.3f, Accuracy: %.3f' % (x_loss, acc))\n",
        "        figsize(5, 5)\n",
        "        plot(losses)\n",
        "        plot(accuracies)\n",
        "\n",
        "\n",
        "holy crap make logsoftmax\n",
        "\n",
        "add shape to tensor, skip Tensor(x[samp]).data.shape\n",
        "\n",
        "error need add mean to tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkCqVatusxHW"
      },
      "source": [
        "## Test NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug9kmXcSv3mh"
      },
      "source": [
        "import requests, gzip, os, hashlib"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C8_VxEEEXA_",
        "outputId": "73f9e863-1fdf-4ad3-d112-7c5323136079"
      },
      "source": [
        "\"\"\"\n",
        "why, because torchvision.datasets.MNIST(root= './', download=True) not work\n",
        "HTTPError: HTTP Error 403: Forbidden\n",
        "\"\"\"\n",
        "def load_data(url : str) -> np.array:\n",
        "    \"\"\"\n",
        "    https://www.geeksforgeeks.org/md5-hash-python/\n",
        "    idea not use name, hash?    \n",
        "    \n",
        "    \"\"\"    \n",
        "    tmp = os.path.join('/content/', hashlib.md5(url.encode('utf-8')).hexdigest())    \n",
        "    if os.path.isfile(tmp):\n",
        "        with open(tmp, 'rb') as f:\n",
        "            d = f.read()\n",
        "    else:\n",
        "        with open(tmp, 'wb') as f:\n",
        "            d = requests.get(url).content\n",
        "            f.write(d)\n",
        "    return np.frombuffer(gzip.decompress(d), dtype = np.uint8)\n",
        "\n",
        "x = load_data(\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\")[16:].reshape((-1, 28, 28))\n",
        "y = load_data(\"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\")[8:]\n",
        "\n",
        "x_test = load_data(\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\")[16:].reshape((-1, 28, 28))\n",
        "y_test = load_data(\"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\")[8:]\n",
        "\n",
        "x.shape, y.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ9tbxXGszCT",
        "outputId": "cbd3fa13-125b-4467-9c12-9ac651c7dc2b"
      },
      "source": [
        "# make nn\n",
        "class NN:\n",
        "    def __init__(self, l1, l2):\n",
        "        self.l1 = Tensor(l1)\n",
        "        self.l2 = Tensor(l2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x.dot(self.l1)\n",
        "        # x = x.relu()\n",
        "        # x = x.dot(self.l2)\n",
        "        # return x.log_softmax()\n",
        "\n",
        "        return x.dot(self.l1).relu().dot(self.l2).log_softmax()\n",
        "\n",
        "# init weight\n",
        "def weight_init(w: int, h: int , mode = 'gauss') -> np.array:\n",
        "    \"\"\"\n",
        "    w : columns, Width.\n",
        "    h : rows, Height.\n",
        "    mode : ['gaussian', 'uniform'] \n",
        "    https://discuss.pytorch.org/t/how-are-layer-weights-and-biases-initialized-by-default/13073   \n",
        "    \"\"\"\n",
        "    if mode == 'gauss':\n",
        "        w = np.random.randn(w, h) / np.sqrt(w*h)\n",
        "    else:\n",
        "        w = np.random.uniform(-1, 1, size = (w, h)) / np.sqrt(w*h)\n",
        "    return w.astype(np.float32)\n",
        "\n",
        "\n",
        "np.random.seed(13)\n",
        "lr = 0.001\n",
        "batch = 30\n",
        "l1 = weight_init(784, 128, mode = 'uniform')\n",
        "l2 = weight_init(128, 10, mode = 'uniform')\n",
        "\n",
        "model = NN(l1, l2)\n",
        "\n",
        "\n",
        "bar = trange(1000)\n",
        "accuracies, losses = [], []\n",
        "for i in bar:\n",
        "    samp = np.random.randint(0, x.shape[0], size=(batch))\n",
        "    X = Tensor(x[samp].reshape((-1, 28*28)))\n",
        "    Y = y[samp]\n",
        "    z = np.zeros((len(samp), 10))\n",
        "    z[range(z.shape[0]), Y] = 1\n",
        "    z = Tensor(z)\n",
        "\n",
        "    y_ = model.forward(X)\n",
        "    # mean\n",
        "    p1 = y_.mul(z)\n",
        "\n",
        "    loss = p1.sum().mul(Tensor(np.array([1/y_.data.size])))\n",
        "    # set_trace()\n",
        "    loss.backward()\n",
        "\n",
        "    cat = np.argmax(y_.data, axis=1)\n",
        "    accuracy = (cat == Y).mean()\n",
        "    # SGD\n",
        "    model.l1.data = model.l1.data - lr*model.l1.grad\n",
        "    model.l2.data = model.l2.data - lr*model.l2.grad\n",
        "    # printing\n",
        "    loss = loss.data\n",
        "    losses.append(loss)\n",
        "    accuracies.append(accuracy)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:221: RuntimeWarning: overflow encountered in exp\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 30/1000 [00:00<00:03, 295.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  5%|▌         | 53/1000 [00:00<00:03, 269.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 84/1000 [00:00<00:03, 279.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 12%|█▏        | 115/1000 [00:00<00:03, 287.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 14%|█▍        | 141/1000 [00:00<00:03, 277.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 17%|█▋        | 169/1000 [00:00<00:02, 277.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 20%|█▉        | 197/1000 [00:00<00:02, 277.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 23%|██▎       | 229/1000 [00:00<00:02, 288.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 26%|██▌       | 258/1000 [00:00<00:02, 288.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 29%|██▉       | 289/1000 [00:01<00:02, 292.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 32%|███▏      | 318/1000 [00:01<00:02, 287.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 35%|███▍      | 347/1000 [00:01<00:02, 276.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 38%|███▊      | 378/1000 [00:01<00:02, 284.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 41%|████      | 407/1000 [00:01<00:02, 285.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 44%|████▎     | 436/1000 [00:01<00:02, 280.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 46%|████▋     | 465/1000 [00:01<00:01, 272.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 50%|████▉     | 495/1000 [00:01<00:01, 280.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 52%|█████▏    | 524/1000 [00:01<00:01, 281.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 56%|█████▌    | 556/1000 [00:01<00:01, 291.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 59%|█████▊    | 586/1000 [00:02<00:01, 280.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 62%|██████▏   | 615/1000 [00:02<00:01, 276.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 64%|██████▍   | 643/1000 [00:02<00:01, 266.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 67%|██████▋   | 673/1000 [00:02<00:01, 275.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 70%|███████   | 703/1000 [00:02<00:01, 281.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 73%|███████▎  | 732/1000 [00:02<00:00, 279.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 76%|███████▋  | 764/1000 [00:02<00:00, 289.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 80%|███████▉  | 795/1000 [00:02<00:00, 293.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 83%|████████▎ | 826/1000 [00:02<00:00, 298.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 86%|████████▌ | 856/1000 [00:03<00:00, 295.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 89%|████████▊ | 886/1000 [00:03<00:00, 293.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 92%|█████████▏| 916/1000 [00:03<00:00, 290.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 95%|█████████▍| 946/1000 [00:03<00:00, 287.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 1000/1000 [00:03<00:00, 285.32it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHfgqmz6D987",
        "outputId": "bfaa020a-e639-4cc6-ffcf-10aedc52a387"
      },
      "source": [
        "model.l2.grad"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[nan, nan, nan, ..., nan, nan, nan],\n",
              "       [nan, nan, nan, ..., nan, nan, nan],\n",
              "       [nan, nan, nan, ..., nan, nan, nan],\n",
              "       ...,\n",
              "       [nan, nan, nan, ..., nan, nan, nan],\n",
              "       [nan, nan, nan, ..., nan, nan, nan],\n",
              "       [nan, nan, nan, ..., nan, nan, nan]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8vjVHAwCyTt",
        "outputId": "67d3977d-2b6a-4763-8cb2-cec1adf9bc25"
      },
      "source": [
        " np.mean(accuracies)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1028"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "fGewSEteH-k5",
        "outputId": "635bbe5e-c2e8-443a-94cd-2416f3d95505"
      },
      "source": [
        "    x_loss = (-out * x_lsm).mean(axis=1)\n",
        "\n",
        "    # compute the gradient\n",
        "    d_out = -out / len(y)\n",
        "    # l2\n",
        "    dx_lsm = d_out - np.exp(x_lsm)*d_out.sum(axis=1).reshape((-1, 1))\n",
        "    d_l2 = x_relu.T.dot(dx_lsm)\n",
        "    # relu\n",
        "    dx_relu = dx_lsm.dot(l2.T)\n",
        "    # l1\n",
        "    dx_l1 = (x_relu > 0).astype(np.float32) * dx_relu\n",
        "    d_l1 = x.T.dot(dx_l1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-23ab16138ea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'int' and 'NoneType'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdMZN7glBSzM",
        "outputId": "67f05847-7b66-4f15-d43d-19032d3c895f"
      },
      "source": [
        "a = np.ones((1,10,10))\n",
        "np.squeeze(a).shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "UgoDZmCNP1TU",
        "outputId": "5c10efdc-e328-48b2-8e91-674d4e04bc57"
      },
      "source": [
        "a, = [[1],[1]]\n",
        "np.ones_like(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-170-74d6ba042ae0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnH2hpFprQTS"
      },
      "source": [
        "model.l1.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB5kJmUP-K57",
        "outputId": "6f42c661-9e68-432f-8f0d-c44a603b955e"
      },
      "source": [
        "if len(Tensor(np.array(1)).data.shape) == 0:"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFbl8uN3oeis",
        "outputId": "151b8967-eee0-4fca-8b01-982659165549"
      },
      "source": [
        "Tensor(np.array(1)).data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOmn1Loez-9H",
        "outputId": "252b9098-95e4-4f06-c221-ec107c5c4e3c"
      },
      "source": [
        "y_.data.size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BQSX2n_KQKv",
        "outputId": "2883ee64-7f1d-42bd-c361-d221b889a2fc"
      },
      "source": [
        "type(Tensor(np.array(72)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.Tensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 228
        }
      ]
    }
  ]
}